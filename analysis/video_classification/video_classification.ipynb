{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_labelling_df = pd.read_csv('videos_with_labelling_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>published</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>category_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Learn what technologies you should learn first...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-19 14:18:42.000000</td>\n",
       "      <td>507722.0</td>\n",
       "      <td>17091.0</td>\n",
       "      <td>0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>729</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Learn about 10 security vulnerabilities every ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-16 14:37:07.000000</td>\n",
       "      <td>62016.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial ‚Äì...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...</td>\n",
       "      <td>Learn how to build a dashboard that generates ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-30 13:32:31.000000</td>\n",
       "      <td>102762.0</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1792</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>Support our campaign here: https://www.freecod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-02 19:00:57.000000</td>\n",
       "      <td>87027.0</td>\n",
       "      <td>3478.0</td>\n",
       "      <td>0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1677</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>JUnit 5 is one of the most popular frameworks ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-12 15:59:45.000000</td>\n",
       "      <td>309188.0</td>\n",
       "      <td>5393.0</td>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1565</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id     video_id  \\\n",
       "0  UC8butISFwT-Wl7EV0hUK0BQ  9He4UBLyk8Y   \n",
       "1  UC8butISFwT-Wl7EV0hUK0BQ  ypNKKYUJE5o   \n",
       "2  UC8butISFwT-Wl7EV0hUK0BQ  D6Xj_W4leu8   \n",
       "3  UC8butISFwT-Wl7EV0hUK0BQ  xZbU6bCZFYo   \n",
       "4  UC8butISFwT-Wl7EV0hUK0BQ  flpmSXVTqBI   \n",
       "\n",
       "                                         video_title  \\\n",
       "0                   Front End Developer Roadmap 2024   \n",
       "1  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2  Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...   \n",
       "3  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4                Java Testing - JUnit 5 Crash Course   \n",
       "\n",
       "                                         description tags  \\\n",
       "0  Learn what technologies you should learn first...  NaN   \n",
       "1  Learn about 10 security vulnerabilities every ...  NaN   \n",
       "2  Learn how to build a dashboard that generates ...  NaN   \n",
       "3  Support our campaign here: https://www.freecod...  NaN   \n",
       "4  JUnit 5 is one of the most popular frameworks ...  NaN   \n",
       "\n",
       "                    published  view_count  like_count  favourite_count  \\\n",
       "0  2023-10-19 14:18:42.000000    507722.0     17091.0                0   \n",
       "1  2023-05-16 14:37:07.000000     62016.0      2625.0                0   \n",
       "2  2023-03-30 13:32:31.000000    102762.0      2133.0                0   \n",
       "3  2021-02-02 19:00:57.000000     87027.0      3478.0                0   \n",
       "4  2021-01-12 15:59:45.000000    309188.0      5393.0                0   \n",
       "\n",
       "   comment_count  duration definition  caption  category_id  \\\n",
       "0          493.0       729         hd    False           27   \n",
       "1           71.0      1505         hd     True           27   \n",
       "2           82.0      1792         hd     True           27   \n",
       "3          197.0      1677         hd     True           27   \n",
       "4           97.0      1565         hd    False           27   \n",
       "\n",
       "                                              prompt classification  \n",
       "0                   Front End Developer Roadmap 2024         Career  \n",
       "1  JavaScript Security Vulnerabilities Tutorial ‚Äì...       Tutorial  \n",
       "2  Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...       Tutorial  \n",
       "3  freeCodeCamp.org Curriculum Expansion: Math + ...           News  \n",
       "4                Java Testing - JUnit 5 Crash Course       Tutorial  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_with_labelling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classification_by_title_df = videos_with_labelling_df[['video_id', 'video_title', 'classification']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  9He4UBLyk8Y                   Front End Developer Roadmap 2024   \n",
       "1  ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2  D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...   \n",
       "3  xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4  flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   \n",
       "\n",
       "  classification  \n",
       "0         Career  \n",
       "1       Tutorial  \n",
       "2       Tutorial  \n",
       "3           News  \n",
       "4       Tutorial  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Tokenization\n",
    "- Tokenization is the process of splitting the text into individual words or tokens. You can use a tokenizer to break down the video titles into their constituent words.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" ‚Üí [\"Front-End\", \"Developer's\", \"Roadmap\", \"2024\", \":\", \"A\", \"Comprehensive\", \"Guide!\"]\n",
    "\n",
    "### NLTK\n",
    "NLTK (Natural Language Toolkit) is a powerful library for natural language processing in Python. It offers various tokenizers for different languages and purposes. Let's delve into NLTK's tokenizers and discuss their suitability for the task of tokenizing video titles.\n",
    "\n",
    "Input Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\"\n",
    "\n",
    "- **Word Tokenization**: \n",
    "  Splits the text into words based on whitespace and punctuation, but keeps contractions and hyphenated words intact. It treats the apostrophe and colon as separate tokens.\n",
    "  Output: ['Front-End', 'Developer', \"'s\", 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "- **WordPunct Tokenization**: \n",
    "  Splits the text into words and punctuation marks, treating each punctuation mark as a separate token. Contractions are split into individual tokens, and hyphenated words are split.\n",
    "  Output: ['Front', '-', 'End', 'Developer', \"'\", 's', 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "- **Regexp Tokenization**: \n",
    "  Uses a regular expression pattern (\\w+) to match alphanumeric characters and underscores. It splits the text into words and numbers, removing other characters like apostrophes and punctuation marks.\n",
    "  Output: ['Front', 'End', 'Developer', 's', 'Roadmap', '2024', 'A', 'Comprehensive', 'Guide']\n",
    "\n",
    "- **Treebank Tokenization**: \n",
    "  Follows the conventions of the Penn Treebank corpus. It treats hyphenated words as single tokens and preserves punctuation marks as separate tokens.\n",
    "  Output: ['Front-End', 'Developer', \"'s\", 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "WordPunct Tokenization may be the best choice for this use case because it preserves punctuation marks, handles contractions and hyphenated words effectively, and provides flexibility in tokenization. Video titles often contain punctuation marks and informal language, making WordPunct Tokenization suitable for maintaining the integrity of the title's structure while extracting meaningful units of text.\n",
    "\n",
    "## Lowercasing\n",
    "- Convert all words in the video titles to lowercase. This ensures that words with different capitalization are treated as the same word.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" ‚Üí \"front-end developer's roadmap 2024: a comprehensive guide!\"\n",
    "\n",
    "## Removing Punctuation\n",
    "- Remove any punctuation marks from the video titles. Punctuation marks such as periods, commas, exclamation marks, etc., are typically not relevant for text classification tasks.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" ‚Üí \"FrontEnd Developers Roadmap 2024 A Comprehensive Guide\"\n",
    "\n",
    "## Removing Stopwords\n",
    "- Stopwords are common words that do not carry much semantic meaning, such as \"and\", \"the\", \"is\", etc. They are often removed because they can introduce noise into the data.\n",
    "- You can use a predefined list of stopwords or a library like NLTK (Natural Language Toolkit) to remove stopwords from the video titles.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" ‚Üí \"Front-End Developer's Roadmap 2024: Comprehensive Guide\"\n",
    "- If there are issues with certificate - try this\n",
    "https://stackoverflow.com/questions/44649449/brew-installation-of-python-3-6-1-ssl-certificate-verify-failed-certificate/44649450#44649450\n",
    "\n",
    "## Handling Special Characters\n",
    "- Depending on the nature of your dataset, you may encounter special characters such as emojis, symbols, or non-alphanumeric characters. Decide whether to keep or remove these characters based on your analysis needs.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide! üòä\" ‚Üí \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\"\n",
    "\n",
    "## Handling Numbers\n",
    "- Decide how to handle numbers in the video titles. You may choose to keep them as-is, remove them, or replace them with placeholders.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" ‚Üí \"Front-End Developer's Roadmap : A Comprehensive Guide!\"\n",
    "\n",
    "## Stemming and Lemmatization: Choosing the Right Technique\n",
    "\n",
    "Stemming and lemmatization are essential text normalization techniques that aim to reduce words to their base or root forms. Both methods are used to enhance the efficiency of text processing and improve the performance of natural language processing (NLP) models. However, they operate differently and have distinct advantages and limitations.\n",
    "\n",
    "### Stemming:\n",
    "\n",
    "Stemming involves removing prefixes or suffixes from words to derive their root forms, known as stems. The goal is to map different variations of a word to the same base form, thereby reducing the dimensionality of the vocabulary. For example, the word \"running\" would be stemmed to \"run\", and \"played\" would be stemmed to \"play\". Stemming algorithms apply heuristic rules to chop off affixes, which may not always produce valid words.\n",
    "\n",
    "### Lemmatization:\n",
    "\n",
    "Lemmatization, on the other hand, maps words to their base or dictionary forms, known as lemmas, by considering the context and meaning of the word. Unlike stemming, lemmatization ensures that the resulting word is valid and meaningful. For example, the word \"ran\" would be lemmatized to \"run\", and \"better\" would be lemmatized to \"good\". Lemmatization relies on linguistic knowledge and requires access to a lexical resource such as WordNet to perform accurate transformations.\n",
    "\n",
    "### Choosing the Right Technique:\n",
    "\n",
    "The choice between stemming and lemmatization depends on the specific requirements of the NLP task and the characteristics of the dataset. Stemming is faster and less computationally intensive, making it suitable for applications where speed is crucial. However, it may produce non-dictionary words or incorrect stems in certain cases. On the other hand, lemmatization ensures the generation of valid words but is slower and requires more computational resources.\n",
    "\n",
    "When deciding between stemming and lemmatization, consider the trade-offs between efficiency and accuracy. In many cases, lemmatization is preferred for tasks requiring precise word normalization and semantic analysis, while stemming may suffice for tasks focused on text classification or information retrieval.\n",
    "\n",
    "Both stemming and lemmatization can be easily implemented using libraries such as NLTK or spaCy, offering flexibility and ease of integration into NLP pipelines. Choose the technique that best aligns with your goals and the characteristics of your dataset to achieve optimal results in your NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "# If there are issues with certificate - try this https://stackoverflow.com/questions/44649449/brew-installation-of-python-3-6-1-ssl-certificate-verify-failed-certificate/44649450#44649450\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess_titles(df, treatments):\n",
    "    # Tokenization\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    df['tokenized_title'] = df['video_title'].apply(tokenizer.tokenize)\n",
    "\n",
    "    # Apply specified treatments\n",
    "    for treatment in treatments:\n",
    "        if treatment == 'lowercasing':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word.lower() for word in x])\n",
    "        elif treatment == 'remove_punctuation':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if re.match(r'^\\w+$', word)])\n",
    "        elif treatment == 'remove_stopwords':\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "        elif treatment == 'remove_special_characters':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [re.sub(r'[^a-zA-Z0-9\\s]', '', word) for word in x])\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word])\n",
    "        elif treatment == 'remove_numbers':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [re.sub(r'\\b\\d+\\b', '', word) for word in x])\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word])\n",
    "        elif treatment == 'stemming':\n",
    "            porter = PorterStemmer()\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [porter.stem(word) for word in x])\n",
    "        elif treatment == 'lemmatization':\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    return df['tokenized_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>classification</th>\n",
       "      <th>tokenized_video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "      <td>[front, end, developer, roadmap, 2024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[javascript, security, vulnerability, tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[use, chatgpt, build, regex, generator, openai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "      <td>[freecodecamp, org, curriculum, expansion, mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[java, testing, junit, 5, crash, course]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>xtges88iZYU</td>\n",
       "      <td>Simplilearn Reviews | Career Restart After Eig...</td>\n",
       "      <td>Career</td>\n",
       "      <td>[simplilearn, review, career, restart, eight, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>gxKzKfWcNww</td>\n",
       "      <td>Top 10 Programming Languages And 10 Highest Pa...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[top, 10, programming, language, 10, highest, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>oB6TC529Oc0</td>\n",
       "      <td>Top 10 Technologies And 10 Highest Paying Jobs...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[top, 10, technology, 10, highest, paying, job...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>hVWrfVlomac</td>\n",
       "      <td>Learn Data Classes In Python In 10 Minutes | H...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[learn, data, class, python, 10, minute, use, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>8s5PlkUEfVw</td>\n",
       "      <td>Top 20 Linux Distros Explained In 13 Minutes |...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[top, 20, linux, distros, explained, 13, minut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                        video_title  \\\n",
       "0     9He4UBLyk8Y                   Front End Developer Roadmap 2024   \n",
       "1     ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2     D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...   \n",
       "3     xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4     flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   \n",
       "...           ...                                                ...   \n",
       "2995  xtges88iZYU  Simplilearn Reviews | Career Restart After Eig...   \n",
       "2996  gxKzKfWcNww  Top 10 Programming Languages And 10 Highest Pa...   \n",
       "2997  oB6TC529Oc0  Top 10 Technologies And 10 Highest Paying Jobs...   \n",
       "2998  hVWrfVlomac  Learn Data Classes In Python In 10 Minutes | H...   \n",
       "2999  8s5PlkUEfVw  Top 20 Linux Distros Explained In 13 Minutes |...   \n",
       "\n",
       "     classification                              tokenized_video_title  \n",
       "0            Career             [front, end, developer, roadmap, 2024]  \n",
       "1          Tutorial  [javascript, security, vulnerability, tutorial...  \n",
       "2          Tutorial  [use, chatgpt, build, regex, generator, openai...  \n",
       "3              News  [freecodecamp, org, curriculum, expansion, mat...  \n",
       "4          Tutorial           [java, testing, junit, 5, crash, course]  \n",
       "...             ...                                                ...  \n",
       "2995         Career  [simplilearn, review, career, restart, eight, ...  \n",
       "2996       Tutorial  [top, 10, programming, language, 10, highest, ...  \n",
       "2997       Tutorial  [top, 10, technology, 10, highest, paying, job...  \n",
       "2998       Tutorial  [learn, data, class, python, 10, minute, use, ...  \n",
       "2999       Tutorial  [top, 20, linux, distros, explained, 13, minut...  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the treatments to apply\n",
    "treatments = ['lowercasing',\n",
    "              'remove_punctuation',\n",
    "              'remove_stopwords',\n",
    "              #'remove_special_characters',\n",
    "              #'remove_numbers',\n",
    "              #'stemming',\n",
    "              'lemmatization']\n",
    "\n",
    "# Apply preprocessing to the DataFrame\n",
    "video_classification_by_title_df['tokenized_video_title'] = preprocess_titles(video_classification_by_title_df.copy(), treatments)\n",
    "\n",
    "video_classification_by_title_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding (if necessary)\n",
    "- Encode the preprocessed text data into a suitable format for further processing or analysis, such as one-hot encoding or word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>classification</th>\n",
       "      <th>tokenized_video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "      <td>[front, end, developer, roadmap, 2024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[javascript, security, vulnerability, tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[use, chatgpt, build, regex, generator, openai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "      <td>[freecodecamp, org, curriculum, expansion, mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[java, testing, junit, 5, crash, course]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  9He4UBLyk8Y                   Front End Developer Roadmap 2024   \n",
       "1  ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2  D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator ‚Äì OpenA...   \n",
       "3  xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4  flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   \n",
       "\n",
       "  classification                              tokenized_video_title  \n",
       "0         Career             [front, end, developer, roadmap, 2024]  \n",
       "1       Tutorial  [javascript, security, vulnerability, tutorial...  \n",
       "2       Tutorial  [use, chatgpt, build, regex, generator, openai...  \n",
       "3           News  [freecodecamp, org, curriculum, expansion, mat...  \n",
       "4       Tutorial           [java, testing, junit, 5, crash, course]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification\n",
       "Tutorial     1629\n",
       "Career        458\n",
       "Project       225\n",
       "Tips          223\n",
       "Challenge     123\n",
       "Review        118\n",
       "News          108\n",
       "Interview     105\n",
       "Lecture         7\n",
       "Debate          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df['classification'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used in information retrieval and text mining to evaluate the importance of a word in a document relative to a collection of documents (corpus). TF-IDF is commonly used for text feature extraction in machine learning and natural language processing tasks.\n",
    "\n",
    "Here's a breakdown of TF-IDF:\n",
    "\n",
    "1. **Term Frequency (TF)**: It measures how frequently a term (word) occurs in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document. The idea is that words that occur more frequently within a document are more important for describing the content of that document.\n",
    "\n",
    "   $$ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: It measures the importance of a term across a collection of documents (corpus). It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. The IDF value decreases as the term appears in more documents, indicating that common terms are less informative than rare terms.\n",
    "\n",
    "   $$ \\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in corpus } |D|}{\\text{Number of documents containing term } t}\\right) $$\n",
    "\n",
    "3. **TF-IDF**: It combines the TF and IDF values to calculate a weighted score for each term in a document. The TF-IDF score increases with the frequency of the term in the document (TF) and decreases with the frequency of the term in the corpus (IDF).\n",
    "\n",
    "   $$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n",
    "\n",
    "In essence, TF-IDF identifies words that are unique and important to a specific document while also considering their general importance across a collection of documents. It's commonly used for tasks like document classification, information retrieval, and text mining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>026</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhou</th>\n",
       "      <th>zip</th>\n",
       "      <th>zod</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>‡§ï‡§∞‡§§</th>\n",
       "      <th>·µê·µíÀ¢·µóÀ° ∏</th>\n",
       "      <th>ùêÇùêéùêÉùêÑ</th>\n",
       "      <th>ùêìùêáùêéùêç</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 3669 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000   01   02  026   03   04   05   06   07   08  ...  zero  zhou  zip  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0  0.0   \n",
       "\n",
       "   zod  zone  zuckerberg  ‡§ï‡§∞‡§§  ·µê·µíÀ¢·µóÀ° ∏  ùêÇùêéùêÉùêÑ  ùêìùêáùêéùêç  \n",
       "0  0.0   0.0         0.0  0.0     0.0   0.0   0.0  \n",
       "1  0.0   0.0         0.0  0.0     0.0   0.0   0.0  \n",
       "2  0.0   0.0         0.0  0.0     0.0   0.0   0.0  \n",
       "3  0.0   0.0         0.0  0.0     0.0   0.0   0.0  \n",
       "4  0.0   0.0         0.0  0.0     0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 3669 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Drop any NaN values in the 'video_title' column\n",
    "video_classification_by_title_df = video_classification_by_title_df.dropna(subset=['video_title'])\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'video_title' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(video_classification_by_title_df['video_title'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DataFrame\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "in            113.614793\n",
       "data          113.386482\n",
       "to            105.033789\n",
       "python        103.521670\n",
       "tutorial       88.412563\n",
       "how            79.583534\n",
       "for            72.793060\n",
       "and            70.296413\n",
       "the            63.687306\n",
       "javascript     61.449447\n",
       "with           58.369572\n",
       "science        54.250774\n",
       "is             53.561014\n",
       "learn          48.970938\n",
       "what           47.112559\n",
       "of             46.376878\n",
       "learning       44.818593\n",
       "analyst        41.672244\n",
       "you            41.642363\n",
       "minutes        40.040516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the TF-IDF scores across all documents\n",
    "tfidf_sum = tfidf_df.sum()\n",
    "\n",
    "# Sort the sums in descending order to get the most important words\n",
    "most_important_words = tfidf_sum.sort_values(ascending=False)\n",
    "\n",
    "# Display the most important words\n",
    "most_important_words.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Career\n",
      "data         44.813518\n",
      "to           31.295703\n",
      "analyst      29.717354\n",
      "how          24.190321\n",
      "science      20.192632\n",
      "become       17.782906\n",
      "in           17.721425\n",
      "scientist    16.833936\n",
      "job          16.104703\n",
      "the          16.064669\n",
      "Name: Career, dtype: float64\n",
      "\n",
      "Classification: Challenge\n",
      "you           7.158587\n",
      "this          5.098004\n",
      "the           4.509462\n",
      "will          4.088448\n",
      "programmer    4.083853\n",
      "are           3.933996\n",
      "daily         3.781269\n",
      "tried         3.702819\n",
      "with          3.128325\n",
      "css           3.027359\n",
      "Name: Challenge, dtype: float64\n",
      "\n",
      "Classification: Debate\n",
      "neuralseek    0.573375\n",
      "technology    0.565979\n",
      "is            0.542158\n",
      "vs            0.516848\n",
      "underrated    0.506866\n",
      "skill         0.488427\n",
      "the           0.463013\n",
      "most          0.408131\n",
      "business      0.404929\n",
      "threat        0.402841\n",
      "Name: Debate, dtype: float64\n",
      "\n",
      "Classification: Interview\n",
      "interview                 5.755735\n",
      "on                        5.090825\n",
      "questions                 3.727423\n",
      "prof                      3.412688\n",
      "data                      3.303274\n",
      "the                       2.919568\n",
      "analyst                   2.668185\n",
      "of                        2.456380\n",
      "interviews                2.332327\n",
      "artificialintelligence    2.190860\n",
      "Name: Interview, dtype: float64\n",
      "\n",
      "Classification: Lecture\n",
      "logic          1.355155\n",
      "computer       1.188051\n",
      "programmers    1.093231\n",
      "do             0.983010\n",
      "terminology    0.771230\n",
      "science        0.588467\n",
      "theory         0.525378\n",
      "atomic         0.510028\n",
      "five           0.510028\n",
      "ws             0.510028\n",
      "Name: Lecture, dtype: float64\n",
      "\n",
      "Classification: News\n",
      "the            6.001171\n",
      "new            4.521609\n",
      "data           4.326657\n",
      "science        4.187100\n",
      "uva            4.135857\n",
      "channel        4.030630\n",
      "datascience    3.514868\n",
      "of             3.390416\n",
      "update         3.343553\n",
      "clip           2.953133\n",
      "Name: News, dtype: float64\n",
      "\n",
      "Classification: Project\n",
      "project    18.706188\n",
      "with       13.635691\n",
      "build      13.336982\n",
      "data       11.322131\n",
      "python      9.533563\n",
      "and         7.680121\n",
      "js          7.381584\n",
      "game        7.237996\n",
      "science     7.108414\n",
      "in          7.057752\n",
      "Name: Project, dtype: float64\n",
      "\n",
      "Classification: Review\n",
      "is         6.694033\n",
      "it         6.021860\n",
      "review     5.236166\n",
      "best       4.841549\n",
      "this       4.633014\n",
      "the        4.531690\n",
      "data       4.076296\n",
      "react      3.685620\n",
      "worth      3.597125\n",
      "courses    3.506497\n",
      "Name: Review, dtype: float64\n",
      "\n",
      "Classification: Tips\n",
      "to        17.140386\n",
      "how       12.198745\n",
      "learn     10.929578\n",
      "you        8.854731\n",
      "fast       8.397718\n",
      "data       7.283140\n",
      "code       7.233798\n",
      "in         6.989195\n",
      "coding     6.171538\n",
      "this       5.812584\n",
      "Name: Tips, dtype: float64\n",
      "\n",
      "Classification: Tutorial\n",
      "python        84.552455\n",
      "tutorial      84.503064\n",
      "in            73.059850\n",
      "javascript    50.492371\n",
      "and           46.938258\n",
      "to            45.109166\n",
      "for           44.873654\n",
      "how           38.036050\n",
      "data          36.227104\n",
      "minutes       34.982021\n",
      "Name: Tutorial, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group tfidf_df by the 'classification' column and calculate the sum of TF-IDF scores for each category\n",
    "tfidf_sum_by_category = tfidf_df.groupby(videos_with_labelling_df['classification']).sum()\n",
    "\n",
    "# Display the top 10 most important words for each category\n",
    "for category, scores in tfidf_sum_by_category.iterrows():\n",
    "    print(f\"Classification: {category}\")\n",
    "    top_10_words = scores.nlargest(10)  # Get the top 10 words with the highest total TF-IDF scores\n",
    "    print(top_10_words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
