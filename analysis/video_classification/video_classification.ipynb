{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "To classify more videos into these categories based on their titles using NLP, you can follow these steps:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Tokenize the video titles: Split each title into individual words or tokens.\n",
    "   - Lowercasing: Convert all tokens to lowercase to ensure consistency.\n",
    "   - Remove punctuation: Eliminate any punctuation marks from the tokens.\n",
    "   - Remove stopwords: Remove common words (e.g., \"the\", \"is\", \"and\") that do not contribute much to the classification.\n",
    "   - Stemming or Lemmatization (optional): Reduce words to their base or root form to further normalize the text data.\n",
    "\n",
    "2. **Feature Extraction:**\n",
    "   - Use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or Bag of Words to convert the preprocessed text data into numerical feature vectors.\n",
    "   - TF-IDF assigns weights to words based on their frequency in the document and inverse frequency across all documents, providing a measure of importance for each word.\n",
    "\n",
    "3. **Model Training and Evaluation:**\n",
    "   - Split the dataset into training and testing sets.\n",
    "   - Train a machine learning model (e.g., Naive Bayes, Logistic Regression, Support Vector Machine, Adaboost Classifier, LSTM) using the training data and the extracted features.\n",
    "   - Evaluate the trained model's performance using the testing data and metrics such as accuracy, precision, recall, and F1-score.\n",
    "   \n",
    "4. **Model Deployment:**\n",
    "   - Once you have a satisfactory model, deploy it to classify new videos into the predefined categories based on their titles.\n",
    "   - You can use the trained model to predict the classification of new video titles.\n",
    "\n",
    "5. **Continuous Improvement:**\n",
    "   - Monitor the model's performance over time and collect feedback.\n",
    "   - Periodically retrain the model with updated data to improve its accuracy and effectiveness.\n",
    "\n",
    "By following these steps, you can build an NLP-based classification system to categorize more videos into the predefined categories based on their titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_labelling_df = pd.read_csv('videos_with_labelling_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_labelling_df = videos_with_labelling_df.rename(columns={'classification': 'video_type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>published</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>category_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>video_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Learn what technologies you should learn first...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-19 14:18:42.000000</td>\n",
       "      <td>507722.0</td>\n",
       "      <td>17091.0</td>\n",
       "      <td>0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>729</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Learn about 10 security vulnerabilities every ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-16 14:37:07.000000</td>\n",
       "      <td>62016.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial –...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator – OpenA...</td>\n",
       "      <td>Learn how to build a dashboard that generates ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-30 13:32:31.000000</td>\n",
       "      <td>102762.0</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1792</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator – OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>Support our campaign here: https://www.freecod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-02 19:00:57.000000</td>\n",
       "      <td>87027.0</td>\n",
       "      <td>3478.0</td>\n",
       "      <td>0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>1677</td>\n",
       "      <td>hd</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC8butISFwT-Wl7EV0hUK0BQ</td>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>JUnit 5 is one of the most popular frameworks ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-12 15:59:45.000000</td>\n",
       "      <td>309188.0</td>\n",
       "      <td>5393.0</td>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1565</td>\n",
       "      <td>hd</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id     video_id  \\\n",
       "0  UC8butISFwT-Wl7EV0hUK0BQ  9He4UBLyk8Y   \n",
       "1  UC8butISFwT-Wl7EV0hUK0BQ  ypNKKYUJE5o   \n",
       "2  UC8butISFwT-Wl7EV0hUK0BQ  D6Xj_W4leu8   \n",
       "3  UC8butISFwT-Wl7EV0hUK0BQ  xZbU6bCZFYo   \n",
       "4  UC8butISFwT-Wl7EV0hUK0BQ  flpmSXVTqBI   \n",
       "\n",
       "                                         video_title  \\\n",
       "0                   Front End Developer Roadmap 2024   \n",
       "1  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2  Use ChatGPT to Build a RegEx Generator – OpenA...   \n",
       "3  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4                Java Testing - JUnit 5 Crash Course   \n",
       "\n",
       "                                         description tags  \\\n",
       "0  Learn what technologies you should learn first...  NaN   \n",
       "1  Learn about 10 security vulnerabilities every ...  NaN   \n",
       "2  Learn how to build a dashboard that generates ...  NaN   \n",
       "3  Support our campaign here: https://www.freecod...  NaN   \n",
       "4  JUnit 5 is one of the most popular frameworks ...  NaN   \n",
       "\n",
       "                    published  view_count  like_count  favourite_count  \\\n",
       "0  2023-10-19 14:18:42.000000    507722.0     17091.0                0   \n",
       "1  2023-05-16 14:37:07.000000     62016.0      2625.0                0   \n",
       "2  2023-03-30 13:32:31.000000    102762.0      2133.0                0   \n",
       "3  2021-02-02 19:00:57.000000     87027.0      3478.0                0   \n",
       "4  2021-01-12 15:59:45.000000    309188.0      5393.0                0   \n",
       "\n",
       "   comment_count  duration definition  caption  category_id  \\\n",
       "0          493.0       729         hd    False           27   \n",
       "1           71.0      1505         hd     True           27   \n",
       "2           82.0      1792         hd     True           27   \n",
       "3          197.0      1677         hd     True           27   \n",
       "4           97.0      1565         hd    False           27   \n",
       "\n",
       "                                              prompt video_type  \n",
       "0                   Front End Developer Roadmap 2024     Career  \n",
       "1  JavaScript Security Vulnerabilities Tutorial –...   Tutorial  \n",
       "2  Use ChatGPT to Build a RegEx Generator – OpenA...   Tutorial  \n",
       "3  freeCodeCamp.org Curriculum Expansion: Math + ...       News  \n",
       "4                Java Testing - JUnit 5 Crash Course   Tutorial  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_with_labelling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_classification_by_title_df = videos_with_labelling_df[['video_id', 'video_title', 'video_type']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator – OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title video_type\n",
       "0  9He4UBLyk8Y                   Front End Developer Roadmap 2024     Career\n",
       "1  ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   Tutorial\n",
       "2  D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator – OpenA...   Tutorial\n",
       "3  xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...       News\n",
       "4  flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   Tutorial"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Tokenization\n",
    "- Tokenization is the process of splitting the text into individual words or tokens. You can use a tokenizer to break down the video titles into their constituent words.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" → [\"Front-End\", \"Developer's\", \"Roadmap\", \"2024\", \":\", \"A\", \"Comprehensive\", \"Guide!\"]\n",
    "\n",
    "### NLTK\n",
    "NLTK (Natural Language Toolkit) is a powerful library for natural language processing in Python. It offers various tokenizers for different languages and purposes. Let's delve into NLTK's tokenizers and discuss their suitability for the task of tokenizing video titles.\n",
    "\n",
    "Input Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\"\n",
    "\n",
    "- **Word Tokenization**: \n",
    "  Splits the text into words based on whitespace and punctuation, but keeps contractions and hyphenated words intact. It treats the apostrophe and colon as separate tokens.\n",
    "  Output: ['Front-End', 'Developer', \"'s\", 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "- **WordPunct Tokenization**: \n",
    "  Splits the text into words and punctuation marks, treating each punctuation mark as a separate token. Contractions are split into individual tokens, and hyphenated words are split.\n",
    "  Output: ['Front', '-', 'End', 'Developer', \"'\", 's', 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "- **Regexp Tokenization**: \n",
    "  Uses a regular expression pattern (\\w+) to match alphanumeric characters and underscores. It splits the text into words and numbers, removing other characters like apostrophes and punctuation marks.\n",
    "  Output: ['Front', 'End', 'Developer', 's', 'Roadmap', '2024', 'A', 'Comprehensive', 'Guide']\n",
    "\n",
    "- **Treebank Tokenization**: \n",
    "  Follows the conventions of the Penn Treebank corpus. It treats hyphenated words as single tokens and preserves punctuation marks as separate tokens.\n",
    "  Output: ['Front-End', 'Developer', \"'s\", 'Roadmap', '2024', ':', 'A', 'Comprehensive', 'Guide', '!']\n",
    "\n",
    "WordPunct Tokenization may be the best choice for this use case because it preserves punctuation marks, handles contractions and hyphenated words effectively, and provides flexibility in tokenization. Video titles often contain punctuation marks and informal language, making WordPunct Tokenization suitable for maintaining the integrity of the title's structure while extracting meaningful units of text.\n",
    "\n",
    "## Lowercasing\n",
    "- Convert all words in the video titles to lowercase. This ensures that words with different capitalization are treated as the same word.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" → \"front-end developer's roadmap 2024: a comprehensive guide!\"\n",
    "\n",
    "## Removing Punctuation\n",
    "- Remove any punctuation marks from the video titles. Punctuation marks such as periods, commas, exclamation marks, etc., are typically not relevant for text classification tasks.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" → \"FrontEnd Developers Roadmap 2024 A Comprehensive Guide\"\n",
    "\n",
    "## Removing Stopwords\n",
    "- Stopwords are common words that do not carry much semantic meaning, such as \"and\", \"the\", \"is\", etc. They are often removed because they can introduce noise into the data.\n",
    "- You can use a predefined list of stopwords or a library like NLTK (Natural Language Toolkit) to remove stopwords from the video titles.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" → \"Front-End Developer's Roadmap 2024: Comprehensive Guide\"\n",
    "- If there are issues with certificate - try this\n",
    "https://stackoverflow.com/questions/44649449/brew-installation-of-python-3-6-1-ssl-certificate-verify-failed-certificate/44649450#44649450\n",
    "\n",
    "## Handling Special Characters\n",
    "- Depending on the nature of your dataset, you may encounter special characters such as emojis, symbols, or non-alphanumeric characters. Decide whether to keep or remove these characters based on your analysis needs.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide! 😊\" → \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\"\n",
    "\n",
    "## Handling Numbers\n",
    "- Decide how to handle numbers in the video titles. You may choose to keep them as-is, remove them, or replace them with placeholders.\n",
    "- Example: \"Front-End Developer's Roadmap 2024: A Comprehensive Guide!\" → \"Front-End Developer's Roadmap : A Comprehensive Guide!\"\n",
    "\n",
    "## Stemming and Lemmatization: Choosing the Right Technique\n",
    "\n",
    "Stemming and lemmatization are essential text normalization techniques that aim to reduce words to their base or root forms. Both methods are used to enhance the efficiency of text processing and improve the performance of natural language processing (NLP) models. However, they operate differently and have distinct advantages and limitations.\n",
    "\n",
    "### Stemming:\n",
    "\n",
    "Stemming involves removing prefixes or suffixes from words to derive their root forms, known as stems. The goal is to map different variations of a word to the same base form, thereby reducing the dimensionality of the vocabulary. For example, the word \"running\" would be stemmed to \"run\", and \"played\" would be stemmed to \"play\". Stemming algorithms apply heuristic rules to chop off affixes, which may not always produce valid words.\n",
    "\n",
    "### Lemmatization:\n",
    "\n",
    "Lemmatization, on the other hand, maps words to their base or dictionary forms, known as lemmas, by considering the context and meaning of the word. Unlike stemming, lemmatization ensures that the resulting word is valid and meaningful. For example, the word \"ran\" would be lemmatized to \"run\", and \"better\" would be lemmatized to \"good\". Lemmatization relies on linguistic knowledge and requires access to a lexical resource such as WordNet to perform accurate transformations.\n",
    "\n",
    "### Choosing the Right Technique:\n",
    "\n",
    "The choice between stemming and lemmatization depends on the specific requirements of the NLP task and the characteristics of the dataset. Stemming is faster and less computationally intensive, making it suitable for applications where speed is crucial. However, it may produce non-dictionary words or incorrect stems in certain cases. On the other hand, lemmatization ensures the generation of valid words but is slower and requires more computational resources.\n",
    "\n",
    "When deciding between stemming and lemmatization, consider the trade-offs between efficiency and accuracy. In many cases, lemmatization is preferred for tasks requiring precise word normalization and semantic analysis, while stemming may suffice for tasks focused on text classification or information retrieval.\n",
    "\n",
    "Both stemming and lemmatization can be easily implemented using libraries such as NLTK or spaCy, offering flexibility and ease of integration into NLP pipelines. Choose the technique that best aligns with your goals and the characteristics of your dataset to achieve optimal results in your NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/harrynorton/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "\n",
    "# If there are issues with certificate - try this https://stackoverflow.com/questions/44649449/brew-installation-of-python-3-6-1-ssl-certificate-verify-failed-certificate/44649450#44649450\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess_titles(df, treatments):\n",
    "    \"\"\"\n",
    "    Preprocesses the video titles in a DataFrame based on the specified treatments.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing the video titles.\n",
    "    - treatments (list): A list of treatments to apply to the video titles. Possible treatments include:\n",
    "                         'lowercasing', 'remove_punctuation', 'remove_stopwords',\n",
    "                         'remove_special_characters', 'remove_numbers', 'stemming', 'lemmatization'.\n",
    "\n",
    "    Returns:\n",
    "    - Series: The preprocessed tokenized titles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    df['tokenized_title'] = df['video_title'].apply(tokenizer.tokenize)\n",
    "\n",
    "    # Apply specified treatments\n",
    "    for treatment in treatments:\n",
    "        if treatment == 'lowercasing':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word.lower() for word in x])\n",
    "        elif treatment == 'remove_punctuation':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if re.match(r'^\\w+$', word)])\n",
    "        elif treatment == 'remove_stopwords':\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "        elif treatment == 'remove_special_characters':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [re.sub(r'[^a-zA-Z0-9\\s]', '', word) for word in x])\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word])\n",
    "        elif treatment == 'remove_numbers':\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [re.sub(r'\\b\\d+\\b', '', word) for word in x])\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [word for word in x if word])\n",
    "        elif treatment == 'stemming':\n",
    "            porter = PorterStemmer()\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [porter.stem(word) for word in x])\n",
    "        elif treatment == 'lemmatization':\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            df['tokenized_title'] = df['tokenized_title'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    return df['tokenized_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_type</th>\n",
       "      <th>tokenized_video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "      <td>[front, end, developer, roadmap, 2024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[javascript, security, vulnerability, tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator – OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[use, chatgpt, build, regex, generator, openai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "      <td>[freecodecamp, org, curriculum, expansion, mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[java, testing, junit, 5, crash, course]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31073</th>\n",
       "      <td>QmPBLroyHB0</td>\n",
       "      <td>Neural Network learns the Mandelbrot set [Part 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[neural, network, learns, mandelbrot, set, par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31074</th>\n",
       "      <td>RO9rfa8-vwo</td>\n",
       "      <td>Life Engine Update (now with graphs! 📈)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[life, engine, update, graph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31075</th>\n",
       "      <td>HpgXTphPCP0</td>\n",
       "      <td>Bugs are Features in Evolution [The Life Engine]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[bug, feature, evolution, life, engine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31076</th>\n",
       "      <td>uGkkm023BSs</td>\n",
       "      <td>Building a Zoo with Evolution [The Life Engine]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[building, zoo, evolution, life, engine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31077</th>\n",
       "      <td>WJyHaPFwFSQ</td>\n",
       "      <td>Evolution of Eyes and Brains [The Life Engine]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[evolution, eye, brain, life, engine]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31078 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                        video_title  \\\n",
       "0      9He4UBLyk8Y                   Front End Developer Roadmap 2024   \n",
       "1      ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   \n",
       "2      D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator – OpenA...   \n",
       "3      xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...   \n",
       "4      flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   \n",
       "...            ...                                                ...   \n",
       "31073  QmPBLroyHB0  Neural Network learns the Mandelbrot set [Part 1]   \n",
       "31074  RO9rfa8-vwo            Life Engine Update (now with graphs! 📈)   \n",
       "31075  HpgXTphPCP0   Bugs are Features in Evolution [The Life Engine]   \n",
       "31076  uGkkm023BSs    Building a Zoo with Evolution [The Life Engine]   \n",
       "31077  WJyHaPFwFSQ     Evolution of Eyes and Brains [The Life Engine]   \n",
       "\n",
       "      video_type                              tokenized_video_title  \n",
       "0         Career             [front, end, developer, roadmap, 2024]  \n",
       "1       Tutorial  [javascript, security, vulnerability, tutorial...  \n",
       "2       Tutorial  [use, chatgpt, build, regex, generator, openai...  \n",
       "3           News  [freecodecamp, org, curriculum, expansion, mat...  \n",
       "4       Tutorial           [java, testing, junit, 5, crash, course]  \n",
       "...          ...                                                ...  \n",
       "31073        NaN  [neural, network, learns, mandelbrot, set, par...  \n",
       "31074        NaN                      [life, engine, update, graph]  \n",
       "31075        NaN            [bug, feature, evolution, life, engine]  \n",
       "31076        NaN           [building, zoo, evolution, life, engine]  \n",
       "31077        NaN              [evolution, eye, brain, life, engine]  \n",
       "\n",
       "[31078 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the treatments to apply\n",
    "treatments = ['lowercasing',\n",
    "              'remove_punctuation',\n",
    "              'remove_stopwords',\n",
    "              #'remove_special_characters',\n",
    "              #'remove_numbers',\n",
    "              #'stemming',\n",
    "              'lemmatization']\n",
    "\n",
    "# Apply preprocessing to the DataFrame\n",
    "video_classification_by_title_df['tokenized_video_title'] = preprocess_titles(video_classification_by_title_df.copy(), treatments)\n",
    "\n",
    "video_classification_by_title_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding (if necessary)\n",
    "- Encode the preprocessed text data into a suitable format for further processing or analysis, such as one-hot encoding or word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_type</th>\n",
       "      <th>tokenized_video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9He4UBLyk8Y</td>\n",
       "      <td>Front End Developer Roadmap 2024</td>\n",
       "      <td>Career</td>\n",
       "      <td>[front, end, developer, roadmap, 2024]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ypNKKYUJE5o</td>\n",
       "      <td>JavaScript Security Vulnerabilities Tutorial  ...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[javascript, security, vulnerability, tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D6Xj_W4leu8</td>\n",
       "      <td>Use ChatGPT to Build a RegEx Generator – OpenA...</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[use, chatgpt, build, regex, generator, openai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xZbU6bCZFYo</td>\n",
       "      <td>freeCodeCamp.org Curriculum Expansion: Math + ...</td>\n",
       "      <td>News</td>\n",
       "      <td>[freecodecamp, org, curriculum, expansion, mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flpmSXVTqBI</td>\n",
       "      <td>Java Testing - JUnit 5 Crash Course</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>[java, testing, junit, 5, crash, course]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title video_type  \\\n",
       "0  9He4UBLyk8Y                   Front End Developer Roadmap 2024     Career   \n",
       "1  ypNKKYUJE5o  JavaScript Security Vulnerabilities Tutorial  ...   Tutorial   \n",
       "2  D6Xj_W4leu8  Use ChatGPT to Build a RegEx Generator – OpenA...   Tutorial   \n",
       "3  xZbU6bCZFYo  freeCodeCamp.org Curriculum Expansion: Math + ...       News   \n",
       "4  flpmSXVTqBI                Java Testing - JUnit 5 Crash Course   Tutorial   \n",
       "\n",
       "                               tokenized_video_title  \n",
       "0             [front, end, developer, roadmap, 2024]  \n",
       "1  [javascript, security, vulnerability, tutorial...  \n",
       "2  [use, chatgpt, build, regex, generator, openai...  \n",
       "3  [freecodecamp, org, curriculum, expansion, mat...  \n",
       "4           [java, testing, junit, 5, crash, course]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_type\n",
       "Tutorial     1629\n",
       "Career        458\n",
       "Project       225\n",
       "Tips          223\n",
       "Challenge     123\n",
       "Review        118\n",
       "News          108\n",
       "Interview     105\n",
       "Lecture         7\n",
       "Debate          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df['video_type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TF-IDF vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic used in information retrieval and text mining to evaluate the importance of a word in a document relative to a collection of documents (corpus). TF-IDF is commonly used for text feature extraction in machine learning and natural language processing tasks.\n",
    "\n",
    "Here's a breakdown of TF-IDF:\n",
    "\n",
    "1. **Term Frequency (TF)**: It measures how frequently a term (word) occurs in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document. The idea is that words that occur more frequently within a document are more important for describing the content of that document.\n",
    "\n",
    "   $$ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: It measures the importance of a term across a collection of documents (corpus). It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. The IDF value decreases as the term appears in more documents, indicating that common terms are less informative than rare terms.\n",
    "\n",
    "   $$ \\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in corpus } |D|}{\\text{Number of documents containing term } t}\\right) $$\n",
    "\n",
    "3. **TF-IDF**: It combines the TF and IDF values to calculate a weighted score for each term in a document. The TF-IDF score increases with the frequency of the term in the document (TF) and decreases with the frequency of the term in the corpus (IDF).\n",
    "\n",
    "   $$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n",
    "\n",
    "In essence, TF-IDF identifies words that are unique and important to a specific document while also considering their general importance across a collection of documents. It's commonly used for tasks like document classification, information retrieval, and text mining."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn TfidfVectorizer\n",
    "1. **Initialize TfidfVectorizer with no preprocessing:** \n",
    "Here, we initialize a TfidfVectorizer object without specifying any preprocessing steps. By setting `preprocessor=None` and `tokenizer=None`, we indicate that we don't want any preprocessing to be applied by the vectorizer. This means that the input data will be used directly as it is without any modifications.\n",
    "\n",
    "2. **Fit and transform the tokenized_video_title to TF-IDF vectors:** \n",
    "We apply the `fit_transform` method of the TfidfVectorizer to convert the tokenized_video_title into TF-IDF vectors. This step computes the TF-IDF values for each word in the tokenized_video_title and represents each document as a vector in the TF-IDF space.\n",
    "\n",
    "3. **Get feature names (words):**\n",
    "After fitting the TfidfVectorizer to the data, we retrieve the feature names, which correspond to the words present in the corpus. These feature names are obtained using the `get_feature_names_out()` method of the TfidfVectorizer.\n",
    "\n",
    "4. **Get TF-IDF values for each document:**\n",
    "We convert the TF-IDF matrix obtained from the fit_transform step into a NumPy array using the `toarray()` method. This array contains the TF-IDF values for each word in each document.\n",
    "\n",
    "5. **Create a DataFrame to store TF-IDF values for each word:**\n",
    "Finally, we create a DataFrame named tfidf_df to store the TF-IDF values for each word. The DataFrame has columns corresponding to the feature names (words) obtained in step 3, and each row represents a document with its corresponding TF-IDF values for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m      8\u001b[0m     analyzer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     tokenizer\u001b[39m=\u001b[39mdummy_fun,\n\u001b[1;32m     10\u001b[0m     preprocessor\u001b[39m=\u001b[39mdummy_fun,\n\u001b[1;32m     11\u001b[0m     token_pattern\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Initialize TfidfVectorizer with no preprocessing\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#tfidf_vectorizer = TfidfVectorizer(preprocessor=None, tokenizer=None)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m# Fit and transform the tokenized_video_title to TF-IDF vectors\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mfit_transform(video_classification_by_title_df[\u001b[39m'\u001b[39;49m\u001b[39mtokenized_video_title\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     19\u001b[0m \u001b[39m# Get feature names (words)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m feature_names \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2133\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2134\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2135\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2136\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[0;32m-> 2138\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2140\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# def dummy_fun(doc):\n",
    "#     return doc\n",
    "\n",
    "# tfidf = TfidfVectorizer(\n",
    "#     analyzer='word',\n",
    "#     tokenizer=dummy_fun,\n",
    "#     preprocessor=dummy_fun,\n",
    "#     token_pattern=None)\n",
    "\n",
    "# Initialize TfidfVectorizer with no preprocessing\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=None, tokenizer=None)\n",
    "\n",
    "# Fit and transform the tokenized_video_title to TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(video_classification_by_title_df['tokenized_video_title'])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get TF-IDF values for each document\n",
    "tfidf_values = tfidf_matrix.toarray()\n",
    "\n",
    "# Create a DataFrame to store TF-IDF values for each word\n",
    "tfidf_df = pd.DataFrame(tfidf_values, columns=feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [front, end, developer, roadmap, 2024]\n",
       "1        [javascript, security, vulnerability, tutorial...\n",
       "2        [use, chatgpt, build, regex, generator, openai...\n",
       "3        [freecodecamp, org, curriculum, expansion, mat...\n",
       "4                 [java, testing, junit, 5, crash, course]\n",
       "                               ...                        \n",
       "31073    [neural, network, learns, mandelbrot, set, par...\n",
       "31074                        [life, engine, update, graph]\n",
       "31075              [bug, feature, evolution, life, engine]\n",
       "31076             [building, zoo, evolution, life, engine]\n",
       "31077                [evolution, eye, brain, life, engine]\n",
       "Name: tokenized_video_title, Length: 31078, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classification_by_title_df['tokenized_video_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          126.395587\n",
       "python        112.624916\n",
       "tutorial      101.351436\n",
       "javascript     66.684754\n",
       "science        59.910169\n",
       "learn          54.663804\n",
       "analyst        52.367814\n",
       "learning       48.864468\n",
       "beginner       44.943550\n",
       "minute         42.785388\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most important words (top TF-IDF words) overall\n",
    "most_important_words_overall = tfidf_df.sum().nlargest(10)\n",
    "\n",
    "# Display the most important words overall\n",
    "most_important_words_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Career\n",
      "data         51.982620\n",
      "analyst      36.005160\n",
      "science      22.835622\n",
      "job          21.629242\n",
      "scientist    20.787768\n",
      "become       20.599251\n",
      "engineer     14.742559\n",
      "get          13.353913\n",
      "developer     9.762521\n",
      "career        9.468605\n",
      "Name: Career, dtype: float64\n",
      "\n",
      "Classification: Challenge\n",
      "interview     4.275692\n",
      "daily         3.930452\n",
      "tried         3.843385\n",
      "programmer    3.796204\n",
      "cs            3.660054\n",
      "mistake       3.215730\n",
      "coding        3.124480\n",
      "ai            3.123360\n",
      "question      3.087153\n",
      "solving       3.076888\n",
      "Name: Challenge, dtype: float64\n",
      "\n",
      "Classification: Debate\n",
      "underrated    0.712834\n",
      "neuralseek    0.644472\n",
      "technology    0.619791\n",
      "tech          0.505279\n",
      "skill         0.486375\n",
      "business      0.451840\n",
      "better        0.442710\n",
      "chatgpt       0.429539\n",
      "threat        0.419043\n",
      "zuckerberg    0.419043\n",
      "Name: Debate, dtype: float64\n",
      "\n",
      "Classification: Interview\n",
      "interview                 8.167957\n",
      "question                  5.255439\n",
      "prof                      3.573410\n",
      "data                      3.498385\n",
      "analyst                   2.801910\n",
      "answer                    2.516170\n",
      "artificialintelligence    2.333142\n",
      "dr                        2.180581\n",
      "ai                        2.166748\n",
      "language                  1.780046\n",
      "Name: Interview, dtype: float64\n",
      "\n",
      "Classification: Lecture\n",
      "computer       1.729577\n",
      "logic          1.420555\n",
      "programmer     1.186857\n",
      "terminology    0.772066\n",
      "actually       0.663451\n",
      "atomic         0.634402\n",
      "five           0.634402\n",
      "science        0.606683\n",
      "theory         0.568990\n",
      "lord           0.509783\n",
      "Name: Lecture, dtype: float64\n",
      "\n",
      "Classification: News\n",
      "new            4.867242\n",
      "data           4.665937\n",
      "science        4.348797\n",
      "uva            4.271488\n",
      "channel        4.223014\n",
      "update         3.800243\n",
      "datascience    3.640706\n",
      "clip           3.068692\n",
      "codebasics     2.735314\n",
      "future         2.697102\n",
      "Name: News, dtype: float64\n",
      "\n",
      "Classification: Project\n",
      "project     23.232511\n",
      "build       14.605212\n",
      "data        11.928844\n",
      "python      10.369419\n",
      "game         8.146632\n",
      "science      7.501062\n",
      "part         6.460010\n",
      "learning     6.057862\n",
      "vlog         5.916869\n",
      "beginner     5.839639\n",
      "Name: Project, dtype: float64\n",
      "\n",
      "Classification: Review\n",
      "best        5.501681\n",
      "review      5.425762\n",
      "data        4.679472\n",
      "worth       4.454303\n",
      "course      4.403171\n",
      "react       4.278220\n",
      "analyst     3.492993\n",
      "new         3.327322\n",
      "coursera    3.065961\n",
      "changer     3.001008\n",
      "Name: Review, dtype: float64\n",
      "\n",
      "Classification: Tips\n",
      "learn      12.719215\n",
      "fast        9.519062\n",
      "code        8.461817\n",
      "data        8.337082\n",
      "tip         8.296601\n",
      "know        7.372513\n",
      "coding      6.979928\n",
      "mistake     6.341937\n",
      "top         6.072545\n",
      "2023        4.993098\n",
      "Name: Tips, dtype: float64\n",
      "\n",
      "Classification: Tutorial\n",
      "tutorial      96.160046\n",
      "python        91.336117\n",
      "javascript    54.518451\n",
      "data          38.918225\n",
      "minute        37.417159\n",
      "beginner      34.897065\n",
      "beau          31.932924\n",
      "teach         31.775855\n",
      "learn         31.560610\n",
      "learning      28.681431\n",
      "Name: Tutorial, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'classification' and calculate the sum of TF-IDF values for each word\n",
    "grouped_tfidf = tfidf_df.groupby(video_classification_by_title_df['video_type']).sum()\n",
    "\n",
    "# Display the most important words for each classification\n",
    "for video_type, tfidf_scores in grouped_tfidf.iterrows():\n",
    "    print(f\"video_type: {video_type}\")\n",
    "    print(tfidf_scores.nlargest(10))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "The order in which you try different classifiers depends on various factors such as the size and nature of your dataset, the complexity of the classification task, and the computational resources available. Here's a suggested order to try these classifiers:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - Logistic Regression is a simple and efficient linear model that serves as a good baseline for classification tasks. It's fast to train and easy to interpret.\n",
    "\n",
    "2. **Naive Bayes:**\n",
    "   - Naive Bayes classifiers are probabilistic models based on Bayes' theorem with the assumption of independence between features. They are particularly effective for text classification tasks and work well with small to medium-sized datasets.\n",
    "\n",
    "3. **Support Vector Machine (SVM):**\n",
    "   - SVM is a powerful supervised learning algorithm capable of handling linear and nonlinear classification tasks. It's effective in high-dimensional spaces and is known for its robustness and flexibility.\n",
    "\n",
    "4. **Adaboost Classifier:**\n",
    "   - Adaboost (Adaptive Boosting) is an ensemble learning method that combines multiple weak classifiers to create a strong classifier. It sequentially corrects the errors of the previous model, making it particularly effective in boosting the performance of other algorithms.\n",
    "\n",
    "5. **LSTM (Long Short-Term Memory):**\n",
    "   - LSTM is a type of recurrent neural network (RNN) architecture commonly used for sequence prediction tasks, including natural language processing (NLP). It's capable of capturing long-term dependencies in sequential data, making it suitable for text classification tasks with complex patterns.\n",
    "\n",
    "Starting with simpler models like Logistic Regression and Naive Bayes allows you to quickly establish a baseline performance and understand the data characteristics. Then, you can gradually explore more complex models like SVM, Adaboost, and LSTM to improve classification accuracy if needed. Additionally, considering the computational complexity of LSTM, it's advisable to try it last, especially if you have limited computational resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to handle any missing values or preprocessing steps before merging and training the model. Additionally, consider performing feature scaling or other data transformations if necessary for better model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Logistic Regression model using the TF-IDF vectors from `tfidf_df` and the `classification` labels from `video_classification_by_title_df`, follow these steps:\n",
    "\n",
    "1. **Merge DataFrames:**\n",
    "   Merge `tfidf_df` with `video_classification_by_title_df` on the common index (assuming the index represents the same order of samples in both DataFrames). This will bring together the TF-IDF vectors and the corresponding classification labels.\n",
    "\n",
    "2. **Split Data:**\n",
    "   Split the merged DataFrame into features (TF-IDF vectors) and labels (classification). The features will be all columns except the `classification` column, and the labels will be the `classification` column.\n",
    "\n",
    "3. **Train/Test Split:**\n",
    "   Split the data into training and testing sets using `train_test_split` from Scikit-learn. This will allow you to train the model on one portion of the data and evaluate its performance on another portion.\n",
    "\n",
    "4. **Initialize Logistic Regression Model:**\n",
    "   Initialize a Logistic Regression model using `LogisticRegression` from Scikit-learn.\n",
    "\n",
    "5. **Train the Model:**\n",
    "   Train the Logistic Regression model using the training data. This is done by calling the `fit` method on the model object with the features and labels of the training set as arguments.\n",
    "\n",
    "6. **Evaluate the Model:**\n",
    "   Evaluate the trained model's performance using the testing data. You can use metrics like accuracy, precision, recall, F1-score, or confusion matrix to assess how well the model performs on unseen data.\n",
    "\n",
    "7. **Tune Hyperparameters (Optional):**\n",
    "   Optionally, you can tune the hyperparameters of the Logistic Regression model using techniques like grid search or random search to optimize its performance.\n",
    "\n",
    "8. **Make Predictions (Optional):**\n",
    "   If you're satisfied with the model's performance, you can use it to make predictions on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (2400, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m logreg_model \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m     17\u001b[0m \u001b[39m# Step 5: Train the Model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m logreg_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Step 6: Evaluate the Model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m accuracy \u001b[39m=\u001b[39m logreg_model\u001b[39m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1201\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1199\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1201\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1202\u001b[0m     X,\n\u001b[1;32m   1203\u001b[0m     y,\n\u001b[1;32m   1204\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1205\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1206\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1207\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1279\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[1;32m   1263\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[0;32m-> 1279\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1283\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1300\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1299\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1300\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1301\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, estimator_name\u001b[39m=\u001b[39mestimator_name)\n\u001b[1;32m   1302\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1367\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1357\u001b[0m             (\n\u001b[1;32m   1358\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1363\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[0;32m-> 1367\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1368\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[1;32m   1369\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (2400, 2) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 1: Merge DataFrames\n",
    "merged_df = pd.concat([tfidf_df, video_classification_by_title_df['classification']], axis=1)\n",
    "\n",
    "# Step 2: Split Data\n",
    "X = merged_df.drop(columns=['classification'])\n",
    "y = merged_df['classification']\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize Logistic Regression Model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Step 5: Train the Model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "accuracy = logreg_model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Tune Hyperparameters (Optional)\n",
    "# (e.g., using GridSearchCV)\n",
    "\n",
    "# Step 8: Make Predictions (Optional)\n",
    "# (e.g., logreg_model.predict(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     classification classification\n",
       "0               0.0         Career\n",
       "1               0.0       Tutorial\n",
       "2               0.0       Tutorial\n",
       "3               0.0           News\n",
       "4               0.0       Tutorial\n",
       "...             ...            ...\n",
       "2995            0.0         Career\n",
       "2996            0.0       Tutorial\n",
       "2997            0.0       Tutorial\n",
       "2998            0.0       Tutorial\n",
       "2999            0.0       Tutorial\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
