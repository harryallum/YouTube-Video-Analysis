{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text\n",
    "import os\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_PROJECT_ID = os.getenv(\"VERTEX_PROJECT_ID\")\n",
    "vertex_region = \"us-west4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all video data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = 'sqlite:///../db/youtube.db'\n",
    "# Create a engine\n",
    "engine = create_engine(db_string)\n",
    "# Create connection\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all video data\n",
    "query = text(\"SELECT * FROM video\")\n",
    "video_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove shorts and very long format videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_videos_by_duration(df, min_duration=60, max_duration=1800):\n",
    "    \"\"\"\n",
    "    Filter videos DataFrame by duration within a specified range.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing video data.\n",
    "        min_duration (int): Minimum duration in seconds. Default is 60.\n",
    "        max_duration (int): Maximum duration in seconds. Default is 1800.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame containing videos with duration within the specified range.\n",
    "    \"\"\"\n",
    "    return df[(df['duration'] >= min_duration) & (df['duration'] <= max_duration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_video_df = filter_videos_by_duration(video_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create traning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "videos_for_labelling_df = filtered_video_df.sample(n=11_000, random_state=37).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gemini prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust display options to prevent truncation\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write prompt string\n",
    "prompt_str = \"\"\"You are a classifier, and I want you to classify each of the following data science youtube video titles based on three overarching categories, video_type, video_topic, coding_language\n",
    "\n",
    "\"video_type\": The only options for video type are: \n",
    "\n",
    "\"Tutorial\",\n",
    "\"Project\",\n",
    "\"News\",\n",
    "\"Tips\",\n",
    "\"Challenge\",\n",
    "\"Career Advice\",\n",
    "\"Podcast/Interview\n",
    "\n",
    "\"video_topic\": The only options for video topic are: \n",
    "\n",
    "\"Statistics and Probability\",\n",
    "\"Machine Learning / AI\",\n",
    "\"Data Wrangling\",\n",
    "\"Data Visualization\",\n",
    "\"Data Mining\",\n",
    "\"Software Engineering\",\n",
    "\"Ethics and Privacy\",\n",
    "\"Cloud Computing\",\n",
    "\"Resume Building\",\n",
    "\"Job Search Strategies\",\n",
    "\"Interview Techniques\",\n",
    "\"Career Development Paths\",\n",
    "\"Balancing Work and Life\",\n",
    "\"Business Acumen\",\n",
    "\n",
    "For video_type and video_topic, you may only use the options provided and no others. If none of these options are appropriate, return an empty string.\n",
    "\n",
    "\"technologies\": also pull out any software/coding_language/packages as a list called 'technologies'. This should not include anything other than software / coding_language / packages though.\n",
    "\n",
    "From now on, return the output in JSON format. Do not include the JSON identifier at the start of the output. Only output pure JSON.\n",
    "\n",
    "The response should look like (for example):\n",
    "\n",
    "{\n",
    "  \"videos\": [\n",
    "    {\n",
    "      \"video_id\": \"YdWkUdMxMvM\",\n",
    "      \"video_title\": \"Career Change to Code - The Complete Guide\",\n",
    "      \"video_info\": {\n",
    "        \"video_type\": \"Career Advice\",\n",
    "        \"video_topic\": \"Software Engineering\",\n",
    "        \"technologies\": []\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"video_id\": \"5rNk7m_zlAg\",\n",
    "      \"video_title\": \"Spring Boot & Spring Data JPA â€“ Complete Course\",\n",
    "      \"video_info\": {\n",
    "        \"video_type\": \"Tutorial\",\n",
    "        \"video_topic\": \"Software Engineering\",\n",
    "        \"technologies\": [\"Spring\", \"Spring Boot\"]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "The video ids and titles are below:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate respone from Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(project_id: str, location: str, prompt) -> str:\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    # Load the model\n",
    "    multimodal_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "    # Query the model\n",
    "    response = multimodal_model.generate_content(\n",
    "        [\n",
    "            prompt\n",
    "        ]\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 rows of 11000...\n",
      "Processed 100 rows of 11000...\n",
      "Processed 150 rows of 11000...\n",
      "Processed 200 rows of 11000...\n",
      "Processed 250 rows of 11000...\n",
      "Processed 300 rows of 11000...\n",
      "Processed 350 rows of 11000...\n",
      "Processed 400 rows of 11000...\n",
      "Processed 450 rows of 11000...\n",
      "Processed 500 rows of 11000...\n",
      "Processed 550 rows of 11000...\n",
      "Processed 600 rows of 11000...\n",
      "Processed 650 rows of 11000...\n",
      "Processed 700 rows of 11000...\n",
      "Processed 750 rows of 11000...\n",
      "Processed 800 rows of 11000...\n",
      "Processed 850 rows of 11000...\n",
      "Processed 900 rows of 11000...\n",
      "Processed 950 rows of 11000...\n",
      "Processed 1000 rows of 11000...\n",
      "Processed 1050 rows of 11000...\n",
      "Processed 1100 rows of 11000...\n",
      "Processed 1150 rows of 11000...\n",
      "Processed 1200 rows of 11000...\n",
      "Processed 1250 rows of 11000...\n",
      "Processed 1300 rows of 11000...\n",
      "Processed 1350 rows of 11000...\n",
      "Processed 1400 rows of 11000...\n",
      "Processed 1450 rows of 11000...\n",
      "Processed 1500 rows of 11000...\n",
      "Error in JSON formatting of chunk 1500, skipping chunk...\n",
      "Processed 1550 rows of 11000...\n",
      "Processed 1600 rows of 11000...\n",
      "Processed 1650 rows of 11000...\n",
      "Processed 1700 rows of 11000...\n",
      "Processed 1750 rows of 11000...\n",
      "Processed 1800 rows of 11000...\n",
      "Processed 1850 rows of 11000...\n",
      "Processed 1900 rows of 11000...\n",
      "Processed 1950 rows of 11000...\n",
      "Processed 2000 rows of 11000...\n",
      "Processed 2050 rows of 11000...\n",
      "Processed 2100 rows of 11000...\n",
      "Processed 2150 rows of 11000...\n",
      "Processed 2200 rows of 11000...\n",
      "Processed 2250 rows of 11000...\n",
      "Processed 2300 rows of 11000...\n",
      "Processed 2350 rows of 11000...\n",
      "Processed 2400 rows of 11000...\n",
      "Processed 2450 rows of 11000...\n",
      "Processed 2500 rows of 11000...\n",
      "Processed 2550 rows of 11000...\n",
      "Processed 2600 rows of 11000...\n",
      "Processed 2650 rows of 11000...\n",
      "Processed 2700 rows of 11000...\n",
      "Processed 2750 rows of 11000...\n",
      "Processed 2800 rows of 11000...\n",
      "Processed 2850 rows of 11000...\n",
      "Processed 2900 rows of 11000...\n",
      "Processed 2950 rows of 11000...\n",
      "Processed 3000 rows of 11000...\n",
      "Processed 3050 rows of 11000...\n",
      "Processed 3100 rows of 11000...\n",
      "Processed 3150 rows of 11000...\n",
      "Processed 3200 rows of 11000...\n",
      "Processed 3250 rows of 11000...\n",
      "Processed 3300 rows of 11000...\n",
      "Processed 3350 rows of 11000...\n",
      "Processed 3400 rows of 11000...\n",
      "Processed 3450 rows of 11000...\n",
      "Processed 3500 rows of 11000...\n",
      "Processed 3550 rows of 11000...\n",
      "Error in JSON formatting of chunk 3550, skipping chunk...\n",
      "Processed 3600 rows of 11000...\n",
      "Processed 3650 rows of 11000...\n",
      "Processed 3700 rows of 11000...\n",
      "Processed 3750 rows of 11000...\n",
      "Processed 3800 rows of 11000...\n",
      "Processed 3850 rows of 11000...\n",
      "Error in JSON formatting of chunk 3850, skipping chunk...\n",
      "Processed 3900 rows of 11000...\n",
      "Processed 3950 rows of 11000...\n",
      "Processed 4000 rows of 11000...\n",
      "Processed 4050 rows of 11000...\n",
      "Processed 4100 rows of 11000...\n",
      "Processed 4150 rows of 11000...\n",
      "Processed 4200 rows of 11000...\n",
      "Processed 4250 rows of 11000...\n",
      "Processed 4300 rows of 11000...\n",
      "Processed 4350 rows of 11000...\n",
      "Processed 4400 rows of 11000...\n",
      "Processed 4450 rows of 11000...\n",
      "Processed 4500 rows of 11000...\n",
      "Processed 4550 rows of 11000...\n",
      "Processed 4600 rows of 11000...\n",
      "Processed 4650 rows of 11000...\n",
      "Processed 4700 rows of 11000...\n",
      "Processed 4750 rows of 11000...\n",
      "Processed 4800 rows of 11000...\n",
      "Processed 4850 rows of 11000...\n",
      "Processed 4900 rows of 11000...\n",
      "Error in JSON formatting of chunk 4900, skipping chunk...\n",
      "Processed 4950 rows of 11000...\n",
      "Processed 5000 rows of 11000...\n",
      "Processed 5050 rows of 11000...\n",
      "Processed 5100 rows of 11000...\n",
      "Processed 5150 rows of 11000...\n",
      "Processed 5200 rows of 11000...\n",
      "Processed 5250 rows of 11000...\n",
      "Processed 5300 rows of 11000...\n",
      "Processed 5350 rows of 11000...\n",
      "Processed 5400 rows of 11000...\n",
      "Processed 5450 rows of 11000...\n",
      "Processed 5500 rows of 11000...\n",
      "Processed 5550 rows of 11000...\n",
      "Processed 5600 rows of 11000...\n",
      "Processed 5650 rows of 11000...\n",
      "Processed 5700 rows of 11000...\n",
      "Processed 5750 rows of 11000...\n",
      "Processed 5800 rows of 11000...\n",
      "Processed 5850 rows of 11000...\n",
      "Processed 5900 rows of 11000...\n",
      "Processed 5950 rows of 11000...\n",
      "Processed 6000 rows of 11000...\n",
      "Processed 6050 rows of 11000...\n",
      "Processed 6100 rows of 11000...\n",
      "Processed 6150 rows of 11000...\n",
      "Processed 6200 rows of 11000...\n",
      "Processed 6250 rows of 11000...\n",
      "Processed 6300 rows of 11000...\n",
      "Processed 6350 rows of 11000...\n",
      "Processed 6400 rows of 11000...\n",
      "Processed 6450 rows of 11000...\n",
      "Processed 6500 rows of 11000...\n",
      "Processed 6550 rows of 11000...\n",
      "Processed 6600 rows of 11000...\n",
      "Processed 6650 rows of 11000...\n",
      "Error in JSON formatting of chunk 6650, skipping chunk...\n",
      "Processed 6700 rows of 11000...\n",
      "Processed 6750 rows of 11000...\n",
      "Processed 6800 rows of 11000...\n",
      "Processed 6850 rows of 11000...\n",
      "Processed 6900 rows of 11000...\n",
      "Processed 6950 rows of 11000...\n",
      "Processed 7000 rows of 11000...\n",
      "Processed 7050 rows of 11000...\n",
      "Processed 7100 rows of 11000...\n",
      "Processed 7150 rows of 11000...\n",
      "Processed 7200 rows of 11000...\n",
      "Processed 7250 rows of 11000...\n",
      "Processed 7300 rows of 11000...\n",
      "Error in JSON formatting of chunk 7300, skipping chunk...\n",
      "Processed 7350 rows of 11000...\n",
      "Processed 7400 rows of 11000...\n",
      "Processed 7450 rows of 11000...\n",
      "Processed 7500 rows of 11000...\n",
      "Processed 7550 rows of 11000...\n",
      "Processed 7600 rows of 11000...\n",
      "Processed 7650 rows of 11000...\n",
      "Processed 7700 rows of 11000...\n",
      "Processed 7750 rows of 11000...\n",
      "Processed 7800 rows of 11000...\n",
      "Processed 7850 rows of 11000...\n",
      "Processed 7900 rows of 11000...\n",
      "Processed 7950 rows of 11000...\n",
      "Processed 8000 rows of 11000...\n",
      "Processed 8050 rows of 11000...\n",
      "Processed 8100 rows of 11000...\n",
      "Processed 8150 rows of 11000...\n",
      "Processed 8200 rows of 11000...\n",
      "Processed 8250 rows of 11000...\n",
      "Processed 8300 rows of 11000...\n",
      "Processed 8350 rows of 11000...\n",
      "Processed 8400 rows of 11000...\n",
      "Processed 8450 rows of 11000...\n",
      "Processed 8500 rows of 11000...\n",
      "Processed 8550 rows of 11000...\n",
      "Processed 8600 rows of 11000...\n",
      "Processed 8650 rows of 11000...\n",
      "Processed 8700 rows of 11000...\n",
      "Processed 8750 rows of 11000...\n",
      "Processed 8800 rows of 11000...\n",
      "Processed 8850 rows of 11000...\n",
      "Processed 8900 rows of 11000...\n",
      "Processed 8950 rows of 11000...\n",
      "Processed 9000 rows of 11000...\n",
      "Processed 9050 rows of 11000...\n",
      "Processed 9100 rows of 11000...\n",
      "Processed 9150 rows of 11000...\n",
      "Processed 9200 rows of 11000...\n",
      "Processed 9250 rows of 11000...\n",
      "Processed 9300 rows of 11000...\n",
      "Processed 9350 rows of 11000...\n",
      "Processed 9400 rows of 11000...\n",
      "Processed 9450 rows of 11000...\n",
      "Processed 9500 rows of 11000...\n",
      "Processed 9550 rows of 11000...\n",
      "Processed 9600 rows of 11000...\n",
      "Processed 9650 rows of 11000...\n",
      "Processed 9700 rows of 11000...\n",
      "Processed 9750 rows of 11000...\n",
      "Processed 9800 rows of 11000...\n",
      "Error in JSON formatting of chunk 9800, skipping chunk...\n",
      "Processed 9850 rows of 11000...\n",
      "Error in JSON formatting of chunk 9850, skipping chunk...\n",
      "Processed 9900 rows of 11000...\n",
      "Processed 9950 rows of 11000...\n",
      "Processed 10000 rows of 11000...\n",
      "Processed 10050 rows of 11000...\n",
      "Processed 10100 rows of 11000...\n",
      "Processed 10150 rows of 11000...\n",
      "Processed 10200 rows of 11000...\n",
      "Processed 10250 rows of 11000...\n",
      "Processed 10300 rows of 11000...\n",
      "Processed 10350 rows of 11000...\n",
      "Processed 10400 rows of 11000...\n",
      "Processed 10450 rows of 11000...\n",
      "Error in JSON formatting of chunk 10450, skipping chunk...\n",
      "Processed 10500 rows of 11000...\n",
      "Processed 10550 rows of 11000...\n",
      "Processed 10600 rows of 11000...\n",
      "Processed 10650 rows of 11000...\n",
      "Processed 10700 rows of 11000...\n",
      "Processed 10750 rows of 11000...\n",
      "Error in JSON formatting of chunk 10750, skipping chunk...\n",
      "Processed 10800 rows of 11000...\n",
      "Processed 10850 rows of 11000...\n",
      "Processed 10900 rows of 11000...\n",
      "Processed 10950 rows of 11000...\n",
      "Processed 11000 rows of 11000...\n",
      "Completed processing\n"
     ]
    }
   ],
   "source": [
    "# Get total number of videos\n",
    "total_videos = len(videos_for_labelling_df)\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 50\n",
    "\n",
    "# Init response list\n",
    "responses = []\n",
    "\n",
    "# Chunk counter\n",
    "chunk_count = 0\n",
    "\n",
    "# Loop through the DataFrame in chunks of 50\n",
    "for chunk_start in range(0, total_videos, chunk_size):\n",
    "    # Get chunk of videos\n",
    "    videos_chunk = videos_for_labelling_df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "\n",
    "    # Init prompt\n",
    "    prompt = prompt_str\n",
    "\n",
    "    # Add video information to prompt\n",
    "    for i, (_, row) in enumerate(videos_chunk.iterrows(), start=1):\n",
    "        video_title = row['video_title']\n",
    "        video_id = row['video_id']\n",
    "        prompt += f'\\nvideo_id : {video_id}, video_title: {video_title}'\n",
    "\n",
    "    # Generate response for the chunk\n",
    "    response_text = generate_response(VERTEX_PROJECT_ID, vertex_region, prompt)\n",
    "\n",
    "    #response_text = response_text.split(\"{\", 1)[1]\n",
    "\n",
    "    # Check if the response ends with \"```\"\n",
    "    if response_text.endswith(\"```\"):\n",
    "        # Remove the ending \"```\"\n",
    "        response_text = response_text[:-3]\n",
    "\n",
    "    # Load with YAML instead of JSON as response normally has trailing comma on last item\n",
    "    try:\n",
    "        video_info_dict = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error in JSON formatting of chunk {chunk_count}, skipping chunk...\")\n",
    "\n",
    "    # Map video_type, video_topic, and technologies to the existing df\n",
    "    for video in video_info_dict['videos']:\n",
    "        video_id = video.get('video_id', '')\n",
    "        \n",
    "        # Get video_info dictionary\n",
    "        video_info = video.get('video_info', {})\n",
    "        \n",
    "        video_type = video_info.get('video_type', '')\n",
    "        video_topic = video_info.get('video_topic', '')\n",
    "        \n",
    "        # Get technologies list\n",
    "        technologies = video_info.get('technologies', [])\n",
    "        technologies_str = ', '.join(technologies) if isinstance(technologies, list) else ''\n",
    "        \n",
    "        # Update the df with the extracted information\n",
    "        videos_for_labelling_df.loc[videos_for_labelling_df['video_id'] == video_id, 'video_type'] = video_type\n",
    "        videos_for_labelling_df.loc[videos_for_labelling_df['video_id'] == video_id, 'video_topic'] = video_topic\n",
    "        videos_for_labelling_df.loc[videos_for_labelling_df['video_id'] == video_id, 'technologies'] = technologies_str\n",
    "\n",
    "    chunk_count += chunk_size\n",
    "\n",
    "    print(f'Processed {chunk_count} rows of {total_videos}...')\n",
    "\n",
    "    # Delay before processing the next chunk\n",
    "    time.sleep(1)\n",
    "\n",
    "print('Completed processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   channel_id       11000 non-null  object \n",
      " 1   video_id         11000 non-null  object \n",
      " 2   video_title      11000 non-null  object \n",
      " 3   description      11000 non-null  object \n",
      " 4   tags             11000 non-null  object \n",
      " 5   published        11000 non-null  object \n",
      " 6   view_count       11000 non-null  float64\n",
      " 7   like_count       10938 non-null  float64\n",
      " 8   favourite_count  11000 non-null  int64  \n",
      " 9   comment_count    10990 non-null  float64\n",
      " 10  duration         11000 non-null  int64  \n",
      " 11  definition       11000 non-null  object \n",
      " 12  caption          11000 non-null  object \n",
      " 13  category_id      11000 non-null  int64  \n",
      " 14  video_type       10313 non-null  object \n",
      " 15  video_topic      10290 non-null  object \n",
      " 16  technologies     10466 non-null  object \n",
      "dtypes: float64(3), int64(3), object(11)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "videos_for_labelling_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_for_labelling_df.to_csv('./training_dataset/labelled_video_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videos_for_labelling_df['channel_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
